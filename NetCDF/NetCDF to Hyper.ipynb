{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NetCDF files to .hyper for use in Tableau\n",
    "\n",
    "Recently, Klaus Schulte asked for some help with wrestling NetCDF files into Tableau.  I wrote up a little script with Python to pull the data out and convert it to a .hyper file for use in Tableau and figured that I should go ahead and document and share the script in case anyone else had a similar problem - so here it is!\n",
    "\n",
    "This Jupyter notebook will walk through the steps of exploring and processing NetCDF files for use in Tableau.\n",
    "\n",
    "A blog post with some more details on the Tableau side of working with the data is [here](fill in the url)\n",
    "\n",
    "Note that this example is a simple NetCDF file, and not all are this straightforward in terms of their structure, but some resources on NetCDF files are linked below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is NetCDF?\n",
    "NetCDF files are a special file format for storing array-oriented scientific data.  \n",
    "\n",
    "Typically you'll have a grid of locations and then a number of attributes associated with each of those locations.  For each attribute at each location you might have multiple measurements or time steps.  The data gets complicated pretty fast!  \n",
    "\n",
    "The [UCAR web site on NetCDF files](https://www.unidata.ucar.edu/software/netcdf/) has all sorts of information about the file format, structure, and use cases!  I highly recommend scrolling through if you have to create or deal with NetCDF files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get to the good stuff, the script!\n",
    "The first thing we need to do is import some useful libraries for working with the data.  In this case, I'm using [Pandas](https://pandas.pydata.org/) and [netCDF4](https://pypi.org/project/netCDF4/).  These are what I will use to read in a NetCDF file and manipulate the data so that it's in a good shape to put into a .hyper file using the [Tableau Hyper API](https://help.tableau.com/current/api/hyper_api/en-us/index.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some useful libraries\n",
    "import netCDF4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's grab some NetCDF files to work with, and then make sure that Python is going to know where to find them.\n",
    "\n",
    "I'm going to work with files from [this site](https://www.ufz.de/index.php?de=37937)\n",
    "\n",
    "Specifically, I'll use this monster huge file [here](https://www.ufz.de/export/data/2/269199_SMI_SM_L02_Oberboden_monatlich_1951-01_2021-12_inv.nc) \n",
    "\n",
    "This really is a large file, so when we get to the processing part of the script I will restrict to just one time step - I'll make a note of it later when we get to that part and will talk about how to expand out to run the whole file.\n",
    "\n",
    "In general, it's a good idea with scripts like this to start small and check to make sure that a small bit of the data is working as expected before you run it on the whole dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where is the data stored?\n",
    "# this assumes the file is in the same directory as the script\n",
    "INPUT_NETCDF = r'269200_SMI_SM_Lall_Gesamtboden_monatlich_1951-01_2021-12_inv.nc'\n",
    "\n",
    "# While we're up here at the start of the script, we might as well set up the info for our output .hyper file too\n",
    "# this will put the output file in the same directory with the input file\n",
    "OUTPUT_HYPER = INPUT_NETCDF[:-3] + '.hyper' # use the name of the input .nc file, but it'll be a .hyper at the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the data!\n",
    "We can use the netCDF4 library to easily read in the data and to explore how it's structured and what the general contents are.\n",
    "\n",
    "Here is a web page that I found helpful for figuring out the netCDF4 library\n",
    "https://unidata.github.io/netcdf4-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the file to a variable named 'nc' \n",
    "nc = netCDF4.Dataset(INPUT_NETCDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the file read in, we can explore the contents.  We can start by just looking at the basic header info - I'll summarize it here, and the next cell in this notebook we will ask for the details to be printed out so that you can see what the info looks like for the file\n",
    "\n",
    "That tells us details on the dimensions in the data:\n",
    "* Time (size: 14)\n",
    "* easting (size: 175)\n",
    "* northing (size: 225)\n",
    "\n",
    "What does this tell us? \n",
    "There are 14 time steps stored in the NetCDF\n",
    "The data is in a 225x175 grid\n",
    "\n",
    "What are the variables?\n",
    "* Time (time)\n",
    "* Lon (northing, easting)\n",
    "* Lat (northing, easting)\n",
    "* Easting (easting)\n",
    "* Northing (northing)\n",
    "* SMI (time, northing, easting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'netCDF4._netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    NCO: netCDF Operators version 4.9.8 (Homepage = http://nco.sf.net, Code = http://github.com/nco/nco)\n",
      "    Conventions: CF-1.8\n",
      "    institution: Helmholtz Center for Environmental Research\n",
      "    originator: UFZ Drought Monitor\n",
      "    contact: klima@ufz.de\n",
      "    crs: EPSG:31468\n",
      "    source: UFZ Drought Monitor / Helmholtz Centre for Environmental Research\n",
      "    creation_date: 2022-08-11\n",
      "    dimensions(sizes): time(852), easting(175), northing(225)\n",
      "    variables(dimensions): int32 time(time), float32 SMI(time, northing, easting), float64 lat(northing, easting), float64 lon(northing, easting)\n",
      "    groups: \n"
     ]
    }
   ],
   "source": [
    "# print out the details for the netcdf file\n",
    "print(nc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Walk through the NetCDF file and grab the attributes we want\n",
    "\n",
    "Since the data is in a big raster, we need to start with knowing how many cells we are dealing with.  \n",
    "\n",
    "Tableau doesn't work with raster data, so we're going to want to pick up a single lat/lng value for each cell and data for whatever variables / time steps are interesting to us in the NetCDF.\n",
    "\n",
    "Let's start with the dimensions for the grid, so we can iterate through and grab the values to drop into our tables in .hyper\n",
    "\n",
    "The goal is to end up with a big file that stores the location, the attribute, and the time step.  It'll be a long list with one row for each cell & time step combination.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "225 175\n"
     ]
    }
   ],
   "source": [
    "# let's get the dimensions for width and height of the grid of data \n",
    "y_size = nc.dimensions['northing'].size # northing coordinate count\n",
    "x_size = nc.dimensions['easting'].size # easting coordinate count\n",
    "print(y_size, x_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up two lists for holding our data\n",
    "# this will make it easy to shovel them into a .hyper file at the end\n",
    "data = [] # this will store all of the data values\n",
    "\n",
    "# this will store the geometry.  Since a NetCDF file is multi-dimensional \n",
    "# (in this case we have a number of time steps) we don't need to repeat the geometry for each time step...\n",
    "# we just need to store it once and have a key to link it to the attributes\n",
    "data_geom = [] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the time consuming part - we're going to walk through the NetCDF file and look at every cell and store the values of interest\n",
    "\n",
    "In this case I'm really making two lists of data:\n",
    "* The Geometry - I don't need to duplicate this for every record, since it would be exactly the same for each time step.  So I just write an ID field and the latitude and longitude and store that to put in a table in the .hyper.  Then I can join or relate the Geometry to the attributes in Tableau.\n",
    "\n",
    "* The data - I will end up with one row in my data table for each cell in the NetCDF raster.  In that row, I'll have an ID field (so I can match it with the geometry table), the time step, and the data value(s) of interest\n",
    "\n",
    "To collect all of this, we just walk through the tables.  In this file, I've simplified to just write the data for 1 time step.  If you want to get the data for ALL of the time steps, you just change the end value for the range in the first FOR statement below (see the comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently collecting data for this time step...\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print('Currently collecting data for this time step...')\n",
    "\n",
    "# grabbing 1 time step for a test - use ** len(times) ** in the for loop instead of hard coding the number \"1\" \n",
    "# as the max value in the loop if you want to go through all of the data instead...\n",
    "for i in range(0, 1): \n",
    "    print(i) # just printing out some info so you know what time step is being processed...delete it if it's too chatty for you\n",
    "    \n",
    "    # walk through every cell in the grid (go through each cell one at a time...just walk along the rows and columns)\n",
    "    for x in range(0, x_size):\n",
    "        for y in range(0, y_size):\n",
    "            # make one list with just id, lat, lng (for the geom table in .hyper)\n",
    "            # we only need to do this once, so we'll just grab the data when we go through the first time step\n",
    "            if i == 0:\n",
    "                data_geom.append([f'{y}-{x}',  # id string for the point location\n",
    "                                  nc.variables['lat'][y][x], # grab the latitude for that row/column \n",
    "                                  nc.variables['lon'][y][x]]) # grab the longitude for that row/column\n",
    "\n",
    "            # don't bother with the masked values (no data value recorded there, it's just an empty cell in the grid)\n",
    "            if nc.variables['SMI'][i].mask[y][x] == False:\n",
    "                data.append([f'{y}-{x}',  #id string for the point location\n",
    "                             nc.variables['time'][i], # the time step ID\n",
    "                             nc.variables['SMI'][i][y][x]]) # the value for that location & time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whew...now that is all done, we can just drop everything into a .hyper file for use in Tableau.\n",
    "\n",
    "If you want details, and sample code, to explain how the Hyper API works, check out the [great documentation](https://help.tableau.com/current/api/hyper_api/en-us/index.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output database is open...\n",
      "Connection to write new .hyper is closed\n"
     ]
    }
   ],
   "source": [
    "# Make the .hyper file\n",
    "from tableauhyperapi import HyperProcess, Telemetry, SqlType, TableDefinition, Connection, CreateMode,TableName, Inserter\n",
    "\n",
    "with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper:\n",
    "        with Connection(endpoint=hyper.endpoint,\n",
    "                        create_mode=CreateMode.CREATE_AND_REPLACE,\n",
    "                        database=OUTPUT_HYPER) as connection:\n",
    "            \n",
    "            print('Output database is open...')\n",
    "\n",
    "            # Create the schemas\n",
    "            connection.catalog.create_schema('Data')\n",
    "            connection.catalog.create_schema('Geom')\n",
    "\n",
    "            # Create the table definitions\n",
    "            schema_data = TableDefinition(table_name=TableName('Data', 'Data'),\n",
    "                                     columns=[\n",
    "                                         TableDefinition.Column('ID', SqlType.text()),\n",
    "                                         TableDefinition.Column('Time', SqlType.double()),\n",
    "                                         TableDefinition.Column('Value', SqlType.double()),\n",
    "                                     ])\n",
    "\n",
    "            schema_geom = TableDefinition(table_name=TableName('Geom', 'Geom'),\n",
    "                                     columns=[\n",
    "                                         TableDefinition.Column('ID', SqlType.text()),\n",
    "                                         TableDefinition.Column('Latitude', SqlType.double()),\n",
    "                                         TableDefinition.Column('Longitude', SqlType.double())\n",
    "                                     ])\n",
    "\n",
    "            # Create the tables in the connection catalog\n",
    "            connection.catalog.create_table(schema_data)\n",
    "            connection.catalog.create_table(schema_geom)\n",
    "\n",
    "            # Insert the data from lists\n",
    "            with Inserter(connection, schema_data) as inserter:\n",
    "                inserter.add_rows(data)\n",
    "                inserter.execute()\n",
    "            with Inserter(connection, schema_geom) as inserter:\n",
    "                inserter.add_rows(data_geom)\n",
    "                inserter.execute()\n",
    "        \n",
    "        print('Connection to write new .hyper is closed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyper",
   "language": "python",
   "name": "hyper"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
